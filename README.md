# Jarvis-Voice-Assistant

# ğŸ§  JARVIS - Offline AI Voice Assistant using LLaMA2 + Ollama

Welcome to **JARVIS**, an intelligent offline voice assistant powered by Meta's **LLaMA2** and running through **Ollama** on your local machine. JARVIS is designed to help with tasks like taking notes, setting reminders, and answering user queries â€” all without requiring an internet connection. Your data stays with you, always.

## ğŸ”¥ Key Features

- ğŸ§  **Local AI Model (LLaMA2 via Ollama)** â€” No internet or cloud needed.
- ğŸ’¬ Accepts text commands (voice can be added optionally).
- ğŸ“ Take and store personal notes in a local `.txt` file.
- â° Set reminders â€” stored locally with timestamps.
- ğŸ” Completely offline and privacy-focused.
- ğŸš€ Fast, customizable, and works with low-latency on modern devices.

---

## ğŸ¯ Project Objective

The main goal behind JARVIS is to develop a fully **offline**, AI-powered assistant that respects user privacy, works without the internet, and is completely free to use. In a world increasingly dependent on cloud-based services, JARVIS shows how advanced AI capabilities can still be achieved **locally**, ensuring user data is never sent elsewhere.

This was developed for academic and research purposes and demonstrates how to integrate LLMs (Large Language Models) with modern Python tools.

---

## ğŸ§  How It Works

- The **LLaMA2** model is run using the **Ollama** platform locally.
- JARVIS takes a user command from the UI (via Streamlit or Flask).
- The command is sent to Ollama's local API (`http://localhost:11434/api/generate`).
- Ollama returns a natural language response from LLaMA2.
- JARVIS then:
  - Saves notes to a local file if the command includes "note".
  - Stores reminders with timestamps if the command includes "remind".
  - Otherwise, simply returns an AI-generated response to the user.

---

## ğŸ› ï¸ Tech Stack

| Tech         | Purpose                          |
|--------------|----------------------------------|
| **Python**   | Main programming language        |
| **Ollama**   | Run LLaMA2 model locally         |
| **LLaMA2**   | AI model for generating replies  |
| **Streamlit**| UI frontend for interaction      |
| **Flask** *(optional)* | Alternate lightweight backend |
| **Local files** | Note/reminder persistence     |

---

## ğŸ“‚ Project Structure

```
jarvis/
â”œâ”€â”€ server.py             # Flask-based backend logic
â”œâ”€â”€ app.py (optional)     # For Streamlit-based frontend
â”œâ”€â”€ notes.txt             # Local notes storage
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html        # Web UI for Flask (if used)
```

---

## ğŸš€ How to Run Locally

> **âš ï¸ JARVIS runs locally using Ollama. Internet is NOT required after setup.**

### ğŸ”§ Step 1: Install Ollama

Download and install from [https://ollama.com](https://ollama.com).

Then, open your terminal and run:
```bash
ollama run llama2
```

This will download and prepare the LLaMA2 model locally.

---

### ğŸ”§ Step 2: Clone This Repo

```bash
git clone https://github.com/your-username/jarvis-ai-assistant.git
cd jarvis-ai-assistant
```

### ğŸ”§ Step 3: Install Python Requirements

```bash
pip install -r requirements.txt
```

---

### ğŸ”§ Step 4: Run JARVIS

You can run with either **Flask** or **Streamlit**:

#### Flask Version:
```bash
python server.py
```
Visit: `http://localhost:5000`

#### Streamlit Version (Recommended for UI):
```bash
streamlit run app.py
```
Visit: Localhost link provided by Streamlit (e.g., `http://localhost:8501`)

---

## ğŸŒ Why Itâ€™s Not Online

JARVIS is intentionally designed to be **offline-first**. Hosting it publicly would require:
- Running Ollama on a cloud server (not supported or permitted for public LLaMA2 deployment).
- Exposing local models online (against the projectâ€™s goal of privacy and offline usage).

> **âš ï¸ Hosting JARVIS online would defeat its core purpose.**

This project is best demonstrated on local machines where privacy and control are paramount.

---

## ğŸ§ª Use Cases

- Take private meeting notes.
- Set personal task reminders without any external sync.
- Ask LLaMA2 basic factual or conversational questions.
- Build an offline productivity tool for students, developers, and researchers.

---

## ğŸ§© Future Enhancements

- ğŸ™ï¸ Add speech-to-text and text-to-speech (voice control).
- ğŸ§  Extend memory using vector stores like ChromaDB or FAISS.
- ğŸ—ƒï¸ GUI application with PyQt or Electron + Python backend.
- ğŸ” Add local encryption for notes and reminders.
- ğŸ“± Package as a mobile/offline desktop app.

---

## ğŸ‘©â€ğŸ« Academic Disclaimer

This project was created as part of a final year/semester academic requirement.

> **Due to the offline-first nature of the project, we are unable to provide a public link, as it relies on a locally hosted LLaMA2 model via Ollama which cannot be deployed online for free or securely.**

---

## ğŸ§‘â€ğŸ’» Author

- **Your Name:** [Your GitHub](https://github.com/Augstee)
- **Email:** augustinepc666@gmail.com

---

## ğŸ“œ License

This project is licensed under the [MIT License](LICENSE).

---

Let me know if you want this pushed into an actual GitHub repo or if you'd like it as a downloadable `.md` file!
